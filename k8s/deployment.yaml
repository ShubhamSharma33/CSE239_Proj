apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
  namespace: llm-inference
spec:
  replicas: 1  # Start with 1 replica
  selector:
    matchLabels:
      app: llm-inference
  template:
    metadata:
      labels:
        app: llm-inference
    spec:
      containers:
      - name: llm-service
        image: shubhamsharma33/llm-inference:v6
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_PATH
          value: "/app/models/llama-3.2-3b-q4.gguf"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        # Optional: Add health checks if your app supports them
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - llm-inference
              topologyKey: kubernetes.io/hostname